{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aging-annex",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "extended-legend",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_filename = '/Users/maxim_rachinskiy/Develop/SE_Java/WiktionaryParser/data/words.txt'\n",
    "samples_filename = '/Users/maxim_rachinskiy/Develop/SE_Java/WiktionaryParser/data/samples.txt'\n",
    "id_prefix = 'wikt_ru'\n",
    "min_gloss_len = 4\n",
    "dev_samples_part = 0.1\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "agricultural-pierce",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_train_samples_filename = f'data/Wiki/{id_prefix}/train_samples.txt'\n",
    "output_dev_samples_filename = f'data/Wiki/{id_prefix}/dev_samples.txt'\n",
    "output_words_filename = f'data/Wiki/{id_prefix}/words.txt'\n",
    "output_glosses_filename = f'data/Wiki/{id_prefix}/glosses.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "settled-attack",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220385, 161809)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(words_filename) as f:\n",
    "    words = json.load(f)\n",
    "    \n",
    "with open(samples_filename) as f:\n",
    "    samples = json.load(f)\n",
    "    \n",
    "len(words), len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "opponent-region",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.replace(u'\\xa0', ' ')\n",
    "    text = text.replace('&nbsp;', ' ')\n",
    "    text = text.replace('<br/>', '\\n')\n",
    "    text = text.replace('<br />', '\\n')\n",
    "    text = text.replace('{{-}}', '-')\n",
    "    text = text.replace('{{L}}', '...')\n",
    "    text = text.replace('{{l}}', '..')\n",
    "    text = text.lstrip(';')\n",
    "    text = text.strip(' ,')\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hollow-porter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'совершенно, абсолютно, ... нисколько не уважать кого-либо; не счи'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(\"; совершенно, абсолютно, {{L}} нисколько не уважать кого-либо; не счи\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "mature-harvey",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_id(word_id):\n",
    "    return id_prefix + '_' + word_id\n",
    "\n",
    "def get_sense_id(word_id, gloss_ind):\n",
    "    return get_word_id(word_id) + f'::{gloss_ind}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "needed-russia",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ids = {}\n",
    "word_id_to_ids = {}\n",
    "id_to_gloss = {}\n",
    "id_to_word = {}\n",
    "\n",
    "for word, glosses in words.items():\n",
    "    current_ids = set()\n",
    "    for i, gloss in enumerate(glosses):\n",
    "        current_id = get_sense_id(word, i)\n",
    "        current_ids.add(current_id)\n",
    "        id_to_gloss[current_id] = gloss\n",
    "        id_to_word[current_id] = word\n",
    "    word_to_ids[word] = current_ids\n",
    "    word_id_to_ids[get_word_id(word)] = current_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "tamil-armenia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quotation': 'Для этого опыта нам потребовался {{выдел|эбонитовый}} стержень.',\n",
       " 'senseLabel': 0,\n",
       " 'targetWordId': 'эбонитовый+ADJECTIVE',\n",
       " 'senseId': 'wikt_ru_эбонитовый+ADJECTIVE::0'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for sample in samples:\n",
    "    sample['senseId'] = get_sense_id(sample['targetWordId'], sample['senseLabel'])\n",
    "    \n",
    "samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "postal-might",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "copyrighted-pizza",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sample(sample_text):\n",
    "    target_words = re.findall(r\"{{выдел\\|([^}]+)}}\", sample_text)\n",
    "    contexts = re.split(r\"{{выдел\\|[^}]+}}\", sample_text)\n",
    "    if len(target_words) == 0:\n",
    "        target_words = re.findall(r\"'''([^']+)'''\", sample_text)\n",
    "        contexts = re.split(r\"'''[^']+'''\", sample_text)\n",
    "        \n",
    "    if len(target_words) == 0:\n",
    "        return None\n",
    "    \n",
    "    tokenized_contexts = [word_tokenize(context) for context in contexts]\n",
    "    result_tokens = []\n",
    "    \n",
    "    for i in range(len(target_words)):\n",
    "        result_tokens.extend((t, False) for t in tokenized_contexts[i])\n",
    "        result_tokens.append((target_words[i], True))\n",
    "    result_tokens.extend((t, False) for t in tokenized_contexts[-1])\n",
    "    \n",
    "    return result_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "defensive-providence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quotation': 'На крутом склоне горы в конце улицы с незапамятных времён высился огромный {{выдел|круглый}} камень.',\n",
       " 'senseLabel': 0,\n",
       " 'targetWordId': 'круглый+ADJECTIVE',\n",
       " 'senseId': 'wikt_ru_круглый+ADJECTIVE::0'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[6676]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "coated-hamburg",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('На', False),\n",
       " ('крутом', False),\n",
       " ('склоне', False),\n",
       " ('горы', False),\n",
       " ('в', False),\n",
       " ('конце', False),\n",
       " ('улицы', False),\n",
       " ('с', False),\n",
       " ('незапамятных', False),\n",
       " ('времён', False),\n",
       " ('высился', False),\n",
       " ('огромный', False),\n",
       " ('круглый', True),\n",
       " ('камень', False),\n",
       " ('.', False)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_sample(samples[6676]['quotation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cutting-invitation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('долгое', False),\n",
       " ('время', False),\n",
       " ('были', False),\n",
       " ('только', False),\n",
       " ('монастыри', False),\n",
       " ('со', False),\n",
       " ('своими', False),\n",
       " ('скрипториями', True),\n",
       " (',', False),\n",
       " ('где', False),\n",
       " ('иногда', False),\n",
       " ('даже', False)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_sample(\"долгое время были только монастыри со своими '''скрипториями''', где иногда даже\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fleet-explosion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3420"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broken_glosses = []\n",
    "\n",
    "for sense_id in id_to_gloss:\n",
    "    id_to_gloss[sense_id] = clean_text(id_to_gloss[sense_id])\n",
    "    if len(id_to_gloss[sense_id]) < min_gloss_len:\n",
    "        broken_glosses.append(sense_id)\n",
    "        \n",
    "for sense_id in broken_glosses:\n",
    "    id_to_gloss.pop(sense_id)\n",
    "    word_to_ids[id_to_word[sense_id]].remove(sense_id)\n",
    "    \n",
    "for word_id in word_id_to_ids:\n",
    "    word_id_to_ids[word_id] = list(word_id_to_ids[word_id])\n",
    "    \n",
    "len(broken_glosses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "surface-discipline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1476\n"
     ]
    }
   ],
   "source": [
    "broken_glosses = set(broken_glosses)\n",
    "cleaned_samples = []\n",
    "\n",
    "for sample in samples:\n",
    "    if sample['senseId'] not in broken_glosses:\n",
    "        cleaned_samples.append(sample)\n",
    "    \n",
    "print(len(samples) - len(cleaned_samples))\n",
    "\n",
    "samples = cleaned_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "absolute-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "statistical-covering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d716b356a824bcb94684cd11d9f7717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "68119"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_samples = []\n",
    "\n",
    "for sample in tqdm(samples):\n",
    "    word = sample['targetWordId']\n",
    "    target_lemma, pos = word.split('+')\n",
    "    quotation = clean_text(sample['quotation'])\n",
    "    tokens = tokenize_sample(quotation)\n",
    "    if tokens is not None and len(word_to_ids[word]) >= 2:\n",
    "        tokenized_samples.append({\n",
    "            'lemma': target_lemma, 'pos': pos,\n",
    "            'sense_id': sample['senseId'], 'tokens': tokens\n",
    "        })\n",
    "        \n",
    "len(tokenized_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "intermediate-treat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lemma': 'эбонитовый',\n",
       " 'pos': 'ADJECTIVE',\n",
       " 'sense_id': 'wikt_ru_эбонитовый+ADJECTIVE::1',\n",
       " 'tokens': [('У', False),\n",
       "  ('неё', False),\n",
       "  ('были', False),\n",
       "  ('две', False),\n",
       "  ('короткохвостые', False),\n",
       "  ('шиншиллы', False),\n",
       "  (':', False),\n",
       "  ('одна', False),\n",
       "  ('эбонитовая', True),\n",
       "  (',', False),\n",
       "  ('а', False),\n",
       "  ('другая', False),\n",
       "  ('—', False),\n",
       "  ('бежевая', False),\n",
       "  ('.', False)]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_samples[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ordered-sullivan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wikt_ru_эбонитовый+ADJECTIVE::0', 'wikt_ru_эбонитовый+ADJECTIVE::1']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_id_to_ids['wikt_ru_эбонитовый+ADJECTIVE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "grave-clearing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'очень чёрный'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_gloss['wikt_ru_эбонитовый+ADJECTIVE::1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "endangered-sodium",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "level-lawrence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61307, 6812)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples, dev_samples, _, _ = train_test_split(\n",
    "    tokenized_samples,\n",
    "    range(len(tokenized_samples)),\n",
    "    test_size=dev_samples_part,\n",
    "    random_state=random_seed\n",
    ")\n",
    "# TODO: rewrite without sklearn\n",
    "\n",
    "len(train_samples), len(dev_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "statutory-victoria",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_train_samples_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_samples, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "with open(output_dev_samples_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(dev_samples, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "with open(output_words_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(word_id_to_ids, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "with open(output_glosses_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(id_to_gloss, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-injection",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
